from fastapi import APIRouter, Depends, HTTPException, status
from sqlmodel import Session, select
from app.models import UserWorkspaceFavorited, Workspace
from app.api.deps import get_db
from datetime import datetime, timezone

router = APIRouter()

@router.post("/workspaces/favorite", summary="Favorite a workspace")
def favorite_workspace(
    workspace_id: str,
    user_id: str,
    db: Session = Depends(get_db)
):
    """
    Mark a workspace as favorite for a user.
    """
    # Check if the workspace exists
    workspace_statement = select(Workspace).where(Workspace.id == workspace_id)
    workspace = db.exec(workspace_statement).first()

    if not workspace:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Workspace not found"
        )

    # Check if the workspace is already favorited by the user
    favorite_check_statement = (
        select(UserWorkspaceFavorited)
        .where(
            (UserWorkspaceFavorited.workspace_id == workspace_id) &
            (UserWorkspaceFavorited.user_id == user_id)
        )
    )
    existing_favorite = db.exec(favorite_check_statement).first()

    if existing_favorite:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Workspace is already favorited by the user"
        )

    # Create a new favorite entry
    new_favorite = UserWorkspaceFavorited(
        user_id=user_id,
        workspace_id=workspace_id,
        favorited_date=datetime.now(timezone.utc)
    )
    db.add(new_favorite)

    # Mark workspace as favorite if applicable (optional)
    workspace.is_favorited = True
    db.add(workspace)

    # Commit changes
    db.commit()
    db.refresh(new_favorite)

    return {
        "status_code": 200,
        "message": "Workspace has been successfully marked as favorite",
        "data": {
            "workspace_id": workspace_id,
            "user_id": user_id,
            "favorited_date": new_favorite.favorited_date
        }
    }











from fastapi import APIRouter, Depends, HTTPException
from sqlmodel import Session, select
from app.models import UserWorkspaceFavorited, Workspace
from app.api.deps import get_db

router = APIRouter()

@router.get("/workspaces/favorites", summary="Get all favorited workspaces for a user")
def get_favorited_workspaces(
    user_id: str,
    db: Session = Depends(get_db)
):
    """
    Retrieve all favorited workspaces for a specific user.
    """
    # Query to get all favorited workspace IDs for the user
    favorited_statement = (
        select(UserWorkspaceFavorited.workspace_id)
        .where(UserWorkspaceFavorited.user_id == user_id)
    )
    favorited_workspaces = db.exec(favorited_statement).all()

    if not favorited_workspaces:
        return {
            "status_code": 200,
            "message": "No favorited workspaces found for the user",
            "data": []
        }

    # Retrieve full workspace details for the favorited workspaces
    workspace_ids = [fav.workspace_id for fav in favorited_workspaces]
    workspaces_statement = select(Workspace).where(Workspace.id.in_(workspace_ids))
    workspaces = db.exec(workspaces_statement).all()

    # Build the response
    response_data = []
    for workspace in workspaces:
        response_data.append({
            "workspace_id": str(workspace.id),
            "workspace_name": workspace.name,
            "is_active": workspace.is_active,
            "is_public": workspace.is_public,
            "created_at": workspace.created,  # Assuming `created` is the creation timestamp
            "updated_at": workspace.last_updated  # Assuming `last_updated` is the update timestamp
        })

    return {
        "status_code": 200,
        "message": "Favorited workspaces retrieved successfully",
        "data": response_data
    }










    # Map admin users to workspaces
    admin_user_map = {}
    for link, user in admin_users_results:
        if link.workspace_id not in admin_user_map:
            admin_user_map[link.workspace_id] = []
        admin_user_map[link.workspace_id].append({
            "user_id": str(user.id),
            "username": user.username
        })

    # Build response
    response_data = []
    for workspace in workspaces:
        response_data.append({
            "workspace_id": str(workspace.id),
            "workspace_name": workspace.name,
            "tags": workspace.tags,
            "description": workspace.description,
            "is_active": workspace.is_active,
            "is_public": workspace.is_public,
            "is_favorited": is_favorite,
            "created_at": workspace.created_date,
            "updated_at": workspace.last_updated_date,
            "admins": admin_user_map.get(workspace.id, [])
        })

    return {
        "status_code": 200,
        "message": "Workspaces retrieved successfully",
        "data": response_data
    }




from fastapi import APIRouter, Depends, Query
from sqlmodel import Session, select
from app.models import User, UserWorkspaceLink, Workspace
from app.api.deps import get_db
from typing import Optional

router = APIRouter()

@router.get("/workspaces/admin", summary="Get all admin workspaces with ILIKE search")
def get_admin_workspaces(
    search: Optional[str] = Query(None, description="Search for users by username"),
    skip: int = Query(0, ge=0, description="Number of records to skip"),
    limit: int = Query(10, gt=0, description="Maximum number of records to return"),
    db: Session = Depends(get_db)
):
    """
    Retrieve all admin workspaces filtered by role and optional username search.
    """
    # Query to get users with the role "workspace_admin"
    statement = (
        select(User, UserWorkspaceLink, Workspace)
        .join(UserWorkspaceLink, User.id == UserWorkspaceLink.user_id)
        .join(Workspace, UserWorkspaceLink.workspace_id == Workspace.id)
        .where(UserWorkspaceLink.role == "workspace_admin")
    )

    # Apply ILIKE search if provided
    if search:
        statement = statement.where(User.username.ilike(f"%{search}%"))

    # Apply pagination
    statement = statement.offset(skip).limit(limit)

    # Execute the query
    results = db.exec(statement).all()

    # Build response
    response_data = []
    for user, link, workspace in results:
        response_data.append({
            "user_id": str(user.id),
            "username": user.username,
            "workspace_id": str(workspace.id),
            "workspace_name": workspace.name,
            "is_active": workspace.is_active,
            "is_public": workspace.is_public,
            "role": link.role
        })

    return {
        "status_code": 200,
        "message": "Admin workspaces retrieved successfully",
        "data": response_data
    }








AutoBI

AutoBI is a powerful and flexible business intelligence tool designed to help users generate insights from their data using natural language queries. This project leverages machine learning models to interpret user queries and generate SQL statements to retrieve the necessary data.

Table of Contents

API Reference

Prerequisites

Installation

Usage

Project Structure

Database Migrations (Alembic)

Contributing

API Reference

For detailed API documentation, please visit the API Docs.

Prerequisites

Python 3.10

PostgreSQL (for database storage)

Docker & Kubernetes (if deploying)

Clone the Repository

To get started, clone the repository:

 git clone https://github.com/your-repo/autobi.git
 cd autobi

Installation

To set up the project locally, follow these steps:

pip install --upgrade pip==23.2.1
pip install poetry==1.8.3
poetry install
pip install pre-commit
pre-commit install --hook-type pre-push

Usage

To start the application, run:

python -m uvicorn src.app.main:app --reload --host 0.0.0.0 --port 8080

Project Structure

AUTOBI
│── backend
│── frontend
│── jobs
│── kube/np
│── .gitignore
│── .gitlab-ci.yml
│── src/
│   ├── app/
│   │   ├── main.py        # Entry point for the FastAPI application
│   │   ├── routers/       # Contains all the route handlers
│   │   ├── utils/         # Utility functions and helpers
│   │   ├── models/        # Database models
│   │   ├── migrations/    # Alembic migration scripts
│   │   ├── core/          # Core business logic
│   │   ├── database.py    # Database connection and session
│   │   ├── config_setup.py # Configuration setup
│   │   ├── app_initializer.py # Initializes components
│   │   ├── tests/         # Unit and integration tests
│── README.md  # Project documentation
│── pyproject.toml  # Poetry configuration

Database Migrations (Alembic)

The AutoBI project uses Alembic for database migrations. To set up and run migrations, follow these steps:

Initialize Alembic (if not already initialized):

alembic init src/migrations

Generate a new migration:

alembic revision --autogenerate -m "describe_changes"

Apply migrations:

alembic upgrade head

Downgrade migrations (if needed):

alembic downgrade -1

Contributing

We welcome contributions to the AutoBI project. To contribute, follow these steps:

Fork the repository.

Create a new branch:

git checkout -b feature/your-feature-name

Make your changes.

Commit your changes:

git commit -m "Add some feature"

Push to the branch:

git push origin feature/your-feature-name

Open a pull request.

Ensure your code adheres to coding standards and includes appropriate tests.

Author: Shivansh Mathur | Incedo Inc.

For further assistance, contact the development team.





Using Alembic in VS Code

If you are using VS Code, follow these steps to efficiently manage Alembic migrations:

1. Open VS Code and Navigate to the Project Directory

Open VS Code and use the built-in terminal to move to your project folder:

cd autobi

2. Activate the Virtual Environment

If using a virtual environment:

source venv/bin/activate  # On macOS/Linux
venv\Scripts\activate  # On Windows

3. Run Alembic Commands in the Terminal

You can execute Alembic commands directly in VS Code's terminal:

Generate a new migration:

alembic revision --autogenerate -m "your_migration_name"

Apply migrations:

alembic upgrade head

Downgrade migrations:

alembic downgrade -1

4. View and Edit Migrations in VS Code

Open src/migrations/versions/ in VS Code.

Select the latest migration file to review or manually adjust the changes before applying.

5. Troubleshooting in VS Code

If Alembic fails to detect changes, ensure your models and database metadata are properly configured.

Run:

alembic check

to verify migrations.

By following these steps, you can seamlessly manage Alembic migrations within VS Code while keeping your database schema in sync with your models.









AutoBI is a self-service platform that enables end users to use natural language processing to translate queries into SQL, allowing users to interact with databases using natural language. The platform serves as the foundation for enabling users to onboard their own data assets (DBs) and contextual metadata into a single northstar platform.

Table of Contents

Overview

Problems We Are Solving

What is In Scope

Use Cases

Common Features

Key Functional Requirements

User Prompt Flow

API Reference

Prerequisites

Installation

Creating a Virtual Environment

Running Database Migrations (Alembic)

Usage

Project Structure

Environment Variables

Database Symbols and Usage

Using Alembic in VS Code

Contributing

Overview

AutoBI is a self-service platform that enables users to translate queries into SQL, leveraging natural language processing. It helps users interact with databases without needing direct SQL knowledge. The platform is designed to unify data assets and contextual metadata.

Problems We Are Solving

Reducing cognitive load on data analysts & power users (Reduction in dashboards, BI reports).

Reducing time spent finding key insights (Quicker insight to action).

Automating decision-making through interactive prompt-based querying.

A single platform to mitigate operational expenses.

What is In Scope

Executive Reporting & Summarization (insight generation).

Self-Service capabilities through the VEGAS portal.

Multi-DB query execution engine (BigQuery, Postgres, Oracle, etc.).

Support for multiple LLM models.

Visualization Auto Generation.

Use Cases

Network Genie

Open platform leveraging Kinetica (Starcoder) for productizing text-to-prompt queries against geospatial data assets.

Supports near real-time query SLAs/SLOs.

AskBI

A natural language-based business insights tool for SQL generation using a modular LLM approach.

Enables executive reports, summaries, and more.

NLQ Market Intelligence

A Market Intelligence NLQ framework leveraging third-party assets.

Helps identify unique opportunities for Verizon by analyzing competitor insights.

Common Features

Need for complex, data-intensive geospatial visualizations & slice-and-dice analytics.

Modular LLM framework for different backends.

Support for BigQuery & common DB platforms (Postgres, MySQL, Oracle).

Support for relatively low latency response times (2-5s).

Easily onboard and rapidly prototype new datasets & data products.

Key Functional Requirements

API Interface Layer

Self-service API deployment (Chatbots, BI dashboards, etc.).

Token-based API key access.

Log, Monitoring & Metrics.

Request throttling.

Service Catalog.

Security & Audit Layer

Audits query prompts before execution.

Query prompt safeguards (SQL injections, volatile prompts, query validation).

Context Definition Layer

Specifies information about dataset tables and rules for referencing them.

LLM uses this to generate SQL that references objects.

LLM Serving Layer

Self-service LLM model selection.

LLM model hosting and versioning.

SQL Execution Layer

Database abstraction layer (Integrates with multiple DB engines).

Database query planner (predicate pushdown).

Query acceleration & caching (low-latency response times).

User Prompt Flow

User Asks a Question → Query is passed through the API Interface Layer.

Context Definition Processing → Uses knowledge repositories & vector embeddings.

Security Checks & Validation → Query is validated against defined safeguards.

LLM Generates SQL → Query is processed through the LLM Serving Layer.

SQL Execution Layer → Query is executed against the relevant DBs.

Feedback Mechanism → Results are reviewed, improving query accuracy over time.

API Reference

For detailed API documentation, please visit the API Docs.

Prerequisites

Python 3.10

PostgreSQL (for database storage)

Docker & Kubernetes (if deploying)

Installation

To set up the project locally, follow these steps:

pip install --upgrade pip==23.2.1
pip install -r requirements.txt
pip install pre-commit
pre-commit install --hook-type pre-push

Creating a Virtual Environment

python -m venv venv
source venv/bin/activate  # On macOS/Linux
venv\Scripts\activate  # On Windows

Running Database Migrations (Alembic)

alembic init src/migrations
alembic revision --autogenerate -m "initial_migration"
alembic upgrade head
alembic check

Usage

python -m uvicorn src.app.main:app --reload --host 0.0.0.0 --port 8080

Environment Variables

Defined in the .env file, including database credentials, API keys, and Celery configurations.

Database Symbols and Usage

PostgreSQL (🐘): Scalable relational database, ideal for structured queries.

MariaDB (🐬): MySQL-compatible DB with performance enhancements.

BigQuery (🔍): Google’s managed, serverless analytics platform for real-time insights.

Using Alembic in VS Code

Use the VS Code terminal to manage migrations with Alembic.

View and edit migration scripts under src/migrations/versions/.

Contributing

To contribute, follow these steps:

git checkout -b feature/your-feature-name
git commit -m "Add new feature"
git push origin feature/your-feature-name

Add your files
